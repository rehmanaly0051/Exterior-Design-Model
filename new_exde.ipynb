{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPx6zlB7R6kz7qWHjqjdwXW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rehmanaly0051/Exterior-Design-Model/blob/main/new_exde.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RgEWpSJlfvKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9447b1-4226-472c-f958-1d2858860fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Exterior Design'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xev0L0AogIzq",
        "outputId": "f81e5a4c-0fe2-447e-8a49-999bfbf490f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Exterior Design\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate safetensors  opencv-python-headless --quiet"
      ],
      "metadata": {
        "id": "dH6QuTMjnjRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dbff88-0382-47e4-a39b-6657280b6b47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "id": "jo1EttxCs8-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_floor_plan(floor_plan_path, output_size=(768, 768)):\n",
        "    \"\"\"Process floor plan image for ControlNet input.\"\"\"\n",
        "    image = cv2.imread(floor_plan_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Could not load image from {floor_plan_path}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize and pad\n",
        "    h, w = image.shape[:2]\n",
        "    ratio = min(output_size[0] / w, output_size[1] / h)\n",
        "    new_size = (int(w * ratio), int(h * ratio))\n",
        "    resized = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.ones((output_size[1], output_size[0], 3), dtype=np.uint8) * 255\n",
        "    offset_x = (output_size[0] - new_size[0]) // 2\n",
        "    offset_y = (output_size[1] - new_size[1]) // 2\n",
        "    canvas[offset_y:offset_y+new_size[1], offset_x:offset_x+new_size[0]] = resized\n",
        "\n",
        "    gray = cv2.cvtColor(canvas, cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "\n",
        "    kernel = np.ones((2, 2), np.uint8)\n",
        "    dilated_edges = cv2.dilate(edges, kernel, iterations=1)\n",
        "\n",
        "    edge_image = cv2.cvtColor(dilated_edges, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    return Image.fromarray(edge_image)\n",
        "\n",
        "# Load models\n",
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"lllyasviel/sd-controlnet-canny\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ")\n",
        "\n",
        "# Setup pipeline\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "# Ask user for floor plan image\n",
        "floor_plan_path = input(\"Enter the path to your floor plan image:\\n\")\n",
        "\n",
        "# Preprocess the floor plan\n",
        "control_image = preprocess_floor_plan(floor_plan_path)\n",
        "\n",
        "# Define automatic prompts for different views\n",
        "views = {\n",
        "    \"Front\": \"Front exterior view of a modern house based on the given floor plan, realistic, daytime\",\n",
        "    \"Back\": \"Back exterior view of a modern house based on the given floor plan, realistic, daytime\",\n",
        "    \"Left\": \"Left side exterior view of a modern house based on the given floor plan, realistic, daytime\",\n",
        "    \"Right\": \"Right side exterior view of a modern house based on the given floor plan, realistic, daytime\",\n",
        "}\n",
        "\n",
        "# Negative prompt to improve quality\n",
        "negative_prompt = \"low quality, blurry, bad architecture, distorted, ugly\"\n",
        "\n",
        "# Output size\n",
        "single_view_size = (768, 768)\n",
        "\n",
        "# Dictionary to store each generated image\n",
        "generated_views = {}\n",
        "\n",
        "# Generate images for each view\n",
        "for view_name, view_prompt in views.items():\n",
        "    seed = random.randint(0, 100000)\n",
        "    generator = torch.manual_seed(seed)\n",
        "\n",
        "    output = pipe(\n",
        "        prompt=view_prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        image=control_image,\n",
        "        num_inference_steps=40,\n",
        "        guidance_scale=7.5,\n",
        "        generator=generator,\n",
        "    )\n",
        "\n",
        "    image = output.images[0]\n",
        "    generated_views[view_name] = image\n",
        "\n",
        "    print(f\"âœ… Generated {view_name} View\")\n",
        "\n",
        "# Create a 2x2 collage\n",
        "collage_width = single_view_size[0] * 2\n",
        "collage_height = single_view_size[1] * 2\n",
        "collage = Image.new('RGB', (collage_width, collage_height), color=(255, 255, 255))\n",
        "\n",
        "# Paste images into collage\n",
        "collage.paste(generated_views[\"Front\"], (0, 0))\n",
        "collage.paste(generated_views[\"Back\"], (single_view_size[0], 0))\n",
        "collage.paste(generated_views[\"Left\"], (0, single_view_size[1]))\n",
        "collage.paste(generated_views[\"Right\"], (single_view_size[0], single_view_size[1]))\n",
        "\n",
        "# Save the final collage\n",
        "final_output_path = \"final_exterior_views.png\"\n",
        "collage.save(final_output_path)\n",
        "print(f\"\\nğŸ¯ All views combined and saved as {final_output_path} successfully!\")"
      ],
      "metadata": {
        "id": "Qkqc-1Synmac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "id": "5r0S0NclpRD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEGIzj4MsD9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}